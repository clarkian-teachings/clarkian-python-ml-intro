{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU87uDXgjPzu"
   },
   "source": [
    "## 1. Download a Sample Video\n",
    "\n",
    "First, let's download a sample video and trim it to 10 seconds to make processing faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hsp3Ln4Yn5r",
    "outputId": "02b35244-ee62-4289-8498-4e16eb8ee6bf"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wget moviepy ultralytics opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QvLdy-LjPzv",
    "outputId": "84b884cb-87c9-4596-aae7-ec7de15c07e4"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wget moviepy ultralytics opencv-python -q\n",
    "\n",
    "# Download a sample traffic video from Pexels\n",
    "!wget \"https://www.pexels.com/download/video/31653765/\" -O original_race_video.mp4\n",
    "\n",
    "# Check if the original video file exists and show its size\n",
    "!ls -lh original_race_video.mp4\n",
    "\n",
    "# Trim the first 10 seconds from the video to make processing faster\n",
    "from moviepy.editor import VideoFileClip\n",
    "print(\"Trimming  first 10 seconds from video...\")\n",
    "\n",
    "# Load the video and create a 10-second clip\n",
    "original_clip = VideoFileClip(\"original_race_video.mp4\")\n",
    "trimmed_clip = original_clip\n",
    "trimmed_clip = original_clip.subclip(10, )  # Remove first 10 seconds\n",
    "\n",
    "# Save the trimmed video\n",
    "trimmed_clip.write_videofile(\"race_video.mp4\", codec=\"libx264\", audio=False)\n",
    "original_clip.close()\n",
    "trimmed_clip.close()\n",
    "\n",
    "# Check the size of the trimmed video\n",
    "!ls -lh race_video.mp4\n",
    "print(\"Video successfully trimmed by 10 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP8PVfd8jPzy"
   },
   "source": [
    "## 2. Import Libraries and Set Up YOLOv8\n",
    "\n",
    "Let's import the necessary libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "489vJjtRjPzy",
    "outputId": "f299c3af-9292-4ffa-f5d5-80c2841ce3dd"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xow7bRzmjPzz"
   },
   "source": [
    "## 3. Load a Pre-trained YOLOv8 Model\n",
    "\n",
    "Now, let's download and load a pre-trained YOLOv8 model. We'll use the YOLOv8n model, which is relatively small and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5_G5p6rjPzz",
    "outputId": "d7e02815-c6b9-4112-9330-bb0173445d66"
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Display model info\n",
    "print(f\"Model loaded: {model}\")\n",
    "print(f\"Model task: {model.task}\")\n",
    "\n",
    "# Classes that YOLOv8 can detect\n",
    "print(\"\\nYOLOv8 can detect these classes:\")\n",
    "for i, class_name in enumerate(model.names.values()):\n",
    "  if i<25:\n",
    "    print(f\"{i}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILg3Ub6IjPz0"
   },
   "source": [
    "Find the class IDs for horses (and animals that might be mistaken for horses) in the list above.\n",
    "- 17: Horse\n",
    "- 19: Cow\n",
    "\n",
    "We'll focus on these classes for our computer vision task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt2w1WSOjPz1"
   },
   "source": [
    "## 4. Detect Horses in One Frame\n",
    "\n",
    "Let's extract a single frame from the video and detect horses in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "aI5ZrJ2YjPz1",
    "outputId": "c8176a70-875b-479c-cbbe-1e632157bab7"
   },
   "outputs": [],
   "source": [
    "conf_threshold = 0.4\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('race_video.mp4')\n",
    "\n",
    "# Check if the video file opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "else:\n",
    "    # Read the first frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Get video details\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        print(f\"Video details:\")\n",
    "        print(f\"Resolution: {frame_width}x{frame_height}\")\n",
    "        print(f\"FPS: {fps}\")\n",
    "        print(f\"Total frames: {total_frames}\")\n",
    "        print(f\"Duration: {total_frames/fps:.2f} seconds\")\n",
    "\n",
    "        # Save the first frame as an image\n",
    "        cv2.imwrite('first_frame.jpg', frame)\n",
    "        print(\"\\nFirst frame saved as 'first_frame.jpg'\")\n",
    "\n",
    "        # Display the frame\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"First frame of the (trimmed) video\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Error: Could not read the first frame\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO11knO6jPz2"
   },
   "source": [
    "Now let's use YOLOv8 to detect horses in the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "QQi6KEOejPz2",
    "outputId": "891d0c51-1ec5-47de-df82-d85c983888fe"
   },
   "outputs": [],
   "source": [
    "# Load the saved first frame\n",
    "frame = cv2.imread('first_frame.jpg')\n",
    "\n",
    "# Define the classes we're interested in (horse)\n",
    "animal_classes = [17]  # COCO dataset class IDs for horse\n",
    "\n",
    "# Run YOLOv8 inference on the frame\n",
    "results = model(frame)\n",
    "\n",
    "# Get detection results from the first detection (there's only one image)\n",
    "result = results[0]\n",
    "\n",
    "# Initialize a list to store horse detections\n",
    "horse_detections = []\n",
    "\n",
    "# Filter for vehicle classes and confidence threshold\n",
    "for box in result.boxes:\n",
    "    class_id = int(box.cls.item())\n",
    "    confidence = box.conf.item()\n",
    "\n",
    "    # Only include vehicles with confidence > conf_threshold\n",
    "    if class_id in animal_classes and confidence > conf_threshold:\n",
    "        # Get bounding box coordinates (convert from PyTorch tensor to integers)\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "\n",
    "        # Add to our vehicle detections list\n",
    "        class_name = model.names[class_id]\n",
    "        horse_detections.append((x1, y1, x2, y2, confidence, class_name))\n",
    "\n",
    "# Create a copy of the frame to draw on\n",
    "annotated_frame = frame.copy()\n",
    "\n",
    "# Draw bounding boxes for vehicle detections\n",
    "for x1, y1, x2, y2, conf, class_name in horse_detections:\n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Prepare label text\n",
    "    label = f\"{class_name}: {conf:.2f}\"\n",
    "\n",
    "    # Determine text size and position\n",
    "    (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    y1 = max(y1, text_height + 5)\n",
    "\n",
    "    # Draw filled rectangle for text background\n",
    "    cv2.rectangle(annotated_frame, (x1, y1 - text_height - 5), (x1 + text_width, y1), (0, 255, 0), -1)\n",
    "\n",
    "    # Add text\n",
    "    cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "# Add a total count to the image\n",
    "total_count = len(horse_detections)\n",
    "count_text = f\"RWWA - Intro to Data Science:  Total Horses: {total_count}\"\n",
    "cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "# Display the annotated frame\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Horse Detection on First Frame\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save the annotated frame\n",
    "cv2.imwrite('annotated_first_frame.jpg', annotated_frame)\n",
    "print(f\"Detected {total_count} horses\")\n",
    "print(\"\\nAnnotated frame saved as 'annotated_first_frame.jpg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7ic0lIsjPz3"
   },
   "source": [
    "## 5 & 6. Process the Entire Video and Show Results\n",
    "\n",
    "Now, let's process the entire video, detect the horses in each frame, and create a new video with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz65iir_a6my"
   },
   "outputs": [],
   "source": [
    "input_path = 'race_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1p29F0ajPz4"
   },
   "outputs": [],
   "source": [
    "# Define a function to process the video\n",
    "def process_video(input_path, output_path, model, target_classes, conf_threshold=0.5):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create video writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize a counter for processed frames\n",
    "    frame_count = 0\n",
    "\n",
    "    # Process frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Update frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Print progress every 10 frames\n",
    "        if frame_count % 10 == 0:\n",
    "            print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
    "\n",
    "        # Run YOLOv8 inference\n",
    "        results = model(frame)\n",
    "        result = results[0]\n",
    "\n",
    "        # Filter for vehicle classes\n",
    "        detections = []\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls.item())\n",
    "            confidence = box.conf.item()\n",
    "\n",
    "            if class_id in target_classes and confidence > conf_threshold:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                class_name = model.names[class_id]\n",
    "                detections.append((x1, y1, x2, y2, confidence, class_name))\n",
    "\n",
    "        # Create a copy of the frame\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        for x1, y1, x2, y2, conf, class_name in detections:\n",
    "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Prepare label text\n",
    "            label = f\"{class_name}: {conf:.2f}\"\n",
    "\n",
    "            # Determine text size and position\n",
    "            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            y1 = max(y1, text_height + 5)\n",
    "\n",
    "            # Draw filled rectangle for text background\n",
    "            cv2.rectangle(annotated_frame, (x1, y1 - text_height - 5), (x1 + text_width, y1), (0, 255, 0), -1)\n",
    "\n",
    "            # Add text\n",
    "            cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "        # Add a total count to the frame\n",
    "        total_count = len(detections)\n",
    "        count_text = f\"RWWA - Intro to Data Science:  Total horses: {total_count}\"\n",
    "        cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Add frame number\n",
    "        frame_text = f\"Frame: {frame_count}/{total_frames}\"\n",
    "        cv2.putText(annotated_frame, frame_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    # Release video capture and writer\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"\\nProcessing complete! Output saved to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzqxL9_FjPz4",
    "outputId": "d2a7adc1-1158-4495-9475-cca2ee03abc1"
   },
   "outputs": [],
   "source": [
    "# Define the target classes (horses)\n",
    "vehicle_classes = [17]  # horse\n",
    "\n",
    "# Process the video (this will be much faster with the 10-second clip)\n",
    "output_path = process_video(\n",
    "    input_path='race_video.mp4',\n",
    "    output_path='annotated_race_video.mp4',\n",
    "    model=model,\n",
    "    target_classes=animal_classes,\n",
    "    conf_threshold=conf_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49q2QtsNjPz5"
   },
   "source": [
    "## Display the Annotated Video\n",
    "\n",
    "Let's display the processed video with animal detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuDv2QAP_Gt2",
    "outputId": "b7b62e6d-8ee0-46b8-fb3e-c5b6d8d84320"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -y -i annotated_race_video.mp4 \\\n",
    "  -c:v libx264 -pix_fmt yuv420p -movflags +faststart \\\n",
    "  -crf 23 -preset veryfast \\\n",
    "  annotated_traffic_video_h264.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "c4JG5iKyjPz5",
    "outputId": "989cf33b-c2c9-496a-ae18-297741d1d69f"
   },
   "outputs": [],
   "source": [
    "# Function to display the video in the notebook\n",
    "def display_video(video_path):\n",
    "    # Read the video file\n",
    "    video_file = open(video_path, \"rb\")\n",
    "    video_bytes = video_file.read()\n",
    "    video_file.close()\n",
    "\n",
    "    # Convert to base64\n",
    "    encoded = b64encode(video_bytes).decode()\n",
    "\n",
    "    # Display the video using HTML\n",
    "    display(HTML(f\"\"\"\n",
    "    <video width=\"800\" height=\"600\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"))\n",
    "\n",
    "# Display the annotated video\n",
    "print(\"Displaying the video with animal detections:\")\n",
    "display_video('annotated_race_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqejhivuJWSc",
    "outputId": "fcd02ceb-89e6-44a1-e9d6-710a7dba610e"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -y -i annotated_race_video.mp4 \\\n",
    "  -c:v libx264 -pix_fmt yuv420p -movflags +faststart \\\n",
    "  -crf 23 -preset veryfast \\\n",
    "  annotated_race_video_h264.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HY9AV2u0JU9Z",
    "outputId": "401f6c2d-b9d7-4e83-8c65-be378a9713d7"
   },
   "outputs": [],
   "source": [
    "!zip -j annotated_race_video_h264.zip /content/annotated_race_video_h264.mp4\n",
    "from google.colab import files\n",
    "files.download(\"annotated_race_video_h264.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7UA4AL6jPz6"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we've learned how to:\n",
    "1. Download a sample video and trim it by 10 seconds for faster processing\n",
    "2. Install YOLOv8 and necessary libraries\n",
    "3. Load a pre-trained model\n",
    "4. Detect horses in a single frame\n",
    "5. Process the entire video\n",
    "6. Display the results with labels and counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK4opyMTjPz6"
   },
   "source": [
    "## Resources and Further Reading\n",
    "\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [OpenCV Documentation](https://docs.opencv.org/)\n",
    "- [COCO Dataset Classes](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/)\n",
    "- [Object Tracking Tutorials](https://learnopencv.com/object-tracking-using-opencv-cpp-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r-ORXp-9yna"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
